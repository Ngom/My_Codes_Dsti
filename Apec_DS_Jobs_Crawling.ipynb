{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a2f956",
   "metadata": {},
   "source": [
    "# Scraping apec webpage (www.apec.fr) with SELENIUM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c83f9",
   "metadata": {},
   "source": [
    "### Web crawling Vs. Web scraping\n",
    "- __Web crawling__, also known as Indexing is used to index the information on the page using bots also known as crawlers. Crawling is essentially what search engines do. It’s all about viewing a page as a whole and indexing it. When a bot crawls a website, it goes through every page and every link, until the last line of the website, looking for ANY information. Web Crawlers are basically used by major search engines like Google, Bing, Yahoo, statistical agencies, and large online aggregators.\n",
    "\n",
    "- __Web scraping__, also known as web data extraction, is similar to web crawling in that it identifies and locates the target data from web pages. __The key difference, is that with web scraping, we know the exact data set identifier__ e.g. an HTML element structure for web pages that are being fixed, from which data needs to be extracted. Web scraping is an automated way of extracting specific data sets using bots which are also known as ‘scrapers’. Once the desired information is collected it can be used for comparison, verification, and analysis based on a given business’s needs and goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a2776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules:\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from time import time, sleep\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "773f1304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the the executable link path\n",
    "link_path = Service('/Users/elhadji/Desktop/Python_Labs/chromedriver')\n",
    "# Creating the “driver”\n",
    "driver =  webdriver.Chrome(service=link_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec04d34f",
   "metadata": {},
   "source": [
    "## Create a function to generate a list urls of webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5a4b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to build the url\n",
    "def build_ful_apecurl(keyword_list, pages_numb):\n",
    "    # Add %20 join strings in keywordlist\n",
    "    str_keyword = '%20'.join(keyword_list)\n",
    "    # set apec web page as variable\n",
    "    root_url = \"https://www.apec.fr/candidat/recherche-emploi.html/emploi?motsCles=\"\n",
    "    # url page\n",
    "    url_page = '&page='\n",
    "    # empty list store full url or each wbepage\n",
    "    apec_url_list = []\n",
    "    # full url by contenating in the following list\n",
    "    for n in pages_numb:\n",
    "        apec_url_list.append(root_url+str_keyword+url_page+str(n))\n",
    "    return apec_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d653ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword to seach : \"data scientist\" as list two items\n",
    "keyword_list = ['data', 'scientist']\n",
    "# page numbers list \n",
    "pages_numb = [n for n in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b2f873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.apec.fr/candidat/recherche-emploi.html/emploi?motsCles=data%20scientist&page=0',\n",
       " 'https://www.apec.fr/candidat/recherche-emploi.html/emploi?motsCles=data%20scientist&page=1',\n",
       " 'https://www.apec.fr/candidat/recherche-emploi.html/emploi?motsCles=data%20scientist&page=2',\n",
       " 'https://www.apec.fr/candidat/recherche-emploi.html/emploi?motsCles=data%20scientist&page=3',\n",
       " 'https://www.apec.fr/candidat/recherche-emploi.html/emploi?motsCles=data%20scientist&page=4']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the list of five pages\n",
    "my_apec_links = build_ful_apecurl(keyword_list, pages_numb)\n",
    "# print to see \n",
    "my_apec_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169319da",
   "metadata": {},
   "source": [
    "## Create function to extract data by tagname, class_name, css_selector, xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f974bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extratct jobs title with driver.find_elements By.TAG_NAME()\n",
    "def job_title_extractor_tagname(url_list, tag):\n",
    "    \"\"\"\n",
    "    Take an url list and a tag and return all jobs the given webpage\n",
    "    \"\"\"\n",
    "    all_jobs = []\n",
    "    # loop thorought urls and extract job title by h2 tag\n",
    "    for url in url_list:\n",
    "        driver.get(url)\n",
    "        all_h2 = driver.find_elements(By.TAG_NAME,tag)\n",
    "        all_jobs += [element.text for element in all_h2]\n",
    "    # return all_jobs\n",
    "    return all_jobs\n",
    "# Probem : It crawl any h2 element in the welpage including job titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7846a131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# job title tag : h2\n",
    "tag = \"h2\"\n",
    "# get all job titles\n",
    "job_titles = job_title_extractor_tagname(my_apec_links, tag)\n",
    "# check the lenght\n",
    "len(job_titles) \n",
    "# Include all h2 elements (job title + other section titles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "873c3450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extratct element with driver.find_elements by.CSS_SELECTOR() >  class_name\n",
    "def job_title_extractor_class(url_list, class_name):\n",
    "    \"\"\"\n",
    "    Take an url list and a class name to return all jobs from all webpages\n",
    "    \"\"\"\n",
    "    all_jobs = []\n",
    "    for url in url_list:\n",
    "        driver.get(url)\n",
    "        all_class = driver.find_elements(By.CSS_SELECTOR,class_name)\n",
    "        all_jobs += [element.text for element in all_class]\n",
    "\n",
    "    return all_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0781ca3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class name valie take two as follow when using by.CSS_SELECTOR()\n",
    "class_name = \".card-title.fs-16\"\n",
    "job_titles = job_title_extractor_class(my_apec_links, class_name) # better than TAG_NAME(), more specific !!!\n",
    "# check the lenght\n",
    "len(job_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43290719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA SCIENTIST F/H\n"
     ]
    }
   ],
   "source": [
    "print(job_titles[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93aa3d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find element by XPATH\n",
    "def job_company_extractor(url_list, class_xpath):\n",
    "    \"\"\"\n",
    "    Take an url and an Xpath to return all job company in a webpage\n",
    "    \"\"\"\n",
    "    all_company = []\n",
    "    for url in url_list:\n",
    "        driver.get(url)\n",
    "        all_comp = driver.find_elements(By.XPATH, class_xpath)\n",
    "        all_company += [element.text for element in all_comp]\n",
    "    #driver.quit()\n",
    "    return all_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d50aa79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define company name xpath\n",
    "class_xpath = \"//*[@class='card-offer__company mb-10']\" # company name\n",
    "# get all the companies\n",
    "job_company = job_company_extractor(my_apec_links, class_xpath)\n",
    "# check the lenght\n",
    "len(job_company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b018b35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STUDIEL PARTICIPATIONS\n"
     ]
    }
   ],
   "source": [
    "print(job_company[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06c035f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defien salary xpath \n",
    "xpath = '//ul[@class=\"details-offer\"]' # one year salary\n",
    "# get all the details\n",
    "job_salary = job_company_extractor(my_apec_links,xpath)\n",
    "# check the lenght\n",
    "len(job_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e2e258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 - 43 k€ brut annuel\n"
     ]
    }
   ],
   "source": [
    "print(job_salary[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a9973ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# job description class name\n",
    "class_name = \".card-offer__description.mb-15\" # salary\n",
    "job_descrition = job_title_extractor_class(my_apec_links, class_name) # using CSS_SELECTOR\n",
    "# check the lenght\n",
    "len(job_descrition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fbd0bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dans le cadre d'une embauche, vous interviendrez en tant que data scientist. Vous analyserez de gros volumes de données de type série temporelle, développerez et déploierez des modèles prédictifs, et assurerez la communication avec le client. Vous contribuer également aux...\n"
     ]
    }
   ],
   "source": [
    "print(job_descrition[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4f01069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Job type, location and date \n",
    "class_name = \".details-offer.important-list\"\n",
    "type_loc_date = job_title_extractor_class(my_apec_links, class_name) # using CSS_SELECTOR\n",
    "# check the lenght\n",
    "len(type_loc_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38c85a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDI\n",
      "Blagnac - 31\n",
      "16/11/2021\n"
     ]
    }
   ],
   "source": [
    "print(type_loc_date[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b36d0eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CDI\\nBlagnac - 31\\n16/11/2021'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_loc_date[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216f6789",
   "metadata": {},
   "source": [
    "### Create one function that can extrat for mutiples classes by xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9d33b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find elementx by XPATH\n",
    "def jobs_infos_extractor(url_list, xpath_class_list):\n",
    "    \"\"\"\n",
    "    Take an url and an Xpath to return all job infos (title, company, salary, description, ....) from each a webpage\n",
    "    \"\"\"\n",
    "    # jobs characteristics\n",
    "    all_jobs = [] # job title\n",
    "    all_company = [] # job company\n",
    "    all_salary = [] # job salary\n",
    "    all_description = [] # job description\n",
    "    all_details = [] # job details (type, location, date)\n",
    "    for url in url_list:\n",
    "        driver.get(url)\n",
    "        # get titles\n",
    "        all_class = driver.find_elements(By.XPATH, xpath_class_list[0])\n",
    "        all_jobs += [element.text for element in all_class]\n",
    "        # get companies\n",
    "        all_comp = driver.find_elements(By.XPATH, xpath_class_list[1])\n",
    "        all_company += [element.text for element in all_comp]\n",
    "        # get salary\n",
    "        all_sal = driver.find_elements(By.XPATH, xpath_class_list[2])\n",
    "        all_salary  += [element.text for element in all_sal]\n",
    "        # get description\n",
    "        all_descrip = driver.find_elements(By.XPATH, xpath_class_list[3])\n",
    "        all_description += [element.text for element in all_descrip]\n",
    "        # get job details\n",
    "        all_det = driver.find_elements(By.XPATH, xpath_class_list[4])\n",
    "        all_details += [element.text for element in all_det]      \n",
    "    # scrapped data now\n",
    "    return (all_jobs, all_company, all_salary, all_description, all_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "055dd563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that splits string into list for job details\n",
    "def string_to_list(text):\n",
    "    return text.split(\"\\n\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2f956f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CDI', 'Blagnac - 31', '16/11/2021']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testinf string to list function\n",
    "txt = 'CDI\\nBlagnac - 31\\n16/11/2021'\n",
    "string_to_list(txt) # It's working well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "541d8ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide type_loc_date into three lists\n",
    "job_det = [string_to_list(my_elem) for my_elem in type_loc_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "323935fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract type \"0\", location \"1\" or date \"2\"\n",
    "type_ = [job_type[0] for job_type in job_det]\n",
    "loc = [job_type[1] for job_type in job_det]\n",
    "dat = [job_type[2] for job_type in job_det]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd48e1d",
   "metadata": {},
   "source": [
    "## Building csv file function form after scraping www.apec.fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "39593227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_apec_csv(keyword_list,xpath_class_list,pages_numb):\n",
    "    \"\"\"\n",
    "    Function that creates a dictionary with all data scrapped from the apec website for data scientist postion\n",
    "    and store un csv file.\n",
    "    \"\"\"\n",
    "    ###################################################################\n",
    "    # builf uls bebpages list\n",
    "    my_apec_links = build_ful_apecurl(keyword_list, pages_numb)\n",
    "    \n",
    "    # Five job items to extract\n",
    "    job_titles, job_company,job_salary,job_descrition,job_details = jobs_infos_extractor(my_apec_links, xpath_class_list)\n",
    "\n",
    "    # Transform job_details to extract type, loc and date\n",
    "    type_loc_date = [string_to_list(my_elem) for my_elem in job_details] # call string_to_list function\n",
    "    # Split type, location and date\n",
    "    contract = [job_type[0] for job_type in type_loc_date]\n",
    "    location = [job_type[1] for job_type in type_loc_date]\n",
    "    date = [job_type[2] for job_type in type_loc_date]\n",
    "    \n",
    "    # create a dictionaye to stucture the data scrapped data\n",
    "    my_dict = {\"Job title\":job_titles, \"Company\":job_company, \"Salary\":job_salary, \"Description\":job_descrition,\n",
    "               \"Contract\":contract, \"Location\":location, \"date\":date}\n",
    "    \n",
    "    # create a data frame\n",
    "    data = pd.DataFrame(my_dict)\n",
    "    \n",
    "    # store data in csv file\n",
    "    data.to_csv('/Users/elhadji/Desktop/Python_Labs/aspec_ds_jobs_offers.csv', sep=\";\", encoding='utf-8', index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ede68df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make keyword list\n",
    "keyword_list = ['data', 'scientist']\n",
    "# page numbers list \n",
    "pages_numb = [n for n in range(21)]\n",
    "# class lists\n",
    "xpath_class_list = ['//h2[@class=\"card-title fs-16\"]', \"//*[@class='card-offer__company mb-10']\",\n",
    "              '//ul[@class=\"details-offer\"]', '//p[@class=\"card-offer__description mb-15\"]', '//ul[@class=\"details-offer important-list\"]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "feb7bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's build a apec offers data \n",
    "build_apec_csv(keyword_list,xpath_class_list,pages_numb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad58f0f",
   "metadata": {},
   "source": [
    "## Exporting scraped data into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3821160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the saved data with pand\n",
    "df = pd.read_csv('/Users/elhadji/Desktop/Python_Labs/aspec_ds_jobs_offers.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9229fcdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Description</th>\n",
       "      <th>Contract</th>\n",
       "      <th>Location</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist F/H</td>\n",
       "      <td>FULL DATA MANAGEMENT</td>\n",
       "      <td>35 - 45 k€ brut annuel</td>\n",
       "      <td>Dans le cadre de notre développement, nous rec...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Lille - 59</td>\n",
       "      <td>16/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist F/H</td>\n",
       "      <td>ROBERT WALTERS FRANCE</td>\n",
       "      <td>45 - 50 k€ brut annuel</td>\n",
       "      <td>Notre client, groupe spécialisé dans l'assuran...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Toulon - 83</td>\n",
       "      <td>10/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist F/H</td>\n",
       "      <td>TALENTS RH</td>\n",
       "      <td>45 - 50 k€ brut annuel</td>\n",
       "      <td>TALENTS RH, société de recrutement spécialisée...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Lille - 59</td>\n",
       "      <td>18/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATA SCIENTIST F/H</td>\n",
       "      <td>STUDIEL PARTICIPATIONS</td>\n",
       "      <td>35 - 43 k€ brut annuel</td>\n",
       "      <td>Dans le cadre d'une embauche, vous interviendr...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Blagnac - 31</td>\n",
       "      <td>16/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist F/H</td>\n",
       "      <td>REGIONSJOB</td>\n",
       "      <td>A négocier</td>\n",
       "      <td>\"Recrutez au delà des compétences.\" PERSUADERS...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Lyon 01 - 69</td>\n",
       "      <td>26/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DATA SCIENTIST F/H</td>\n",
       "      <td>PREM CANAGARADJA</td>\n",
       "      <td>A négocier</td>\n",
       "      <td>D’une manière générale, vous serez en charge d...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Toulouse - 31</td>\n",
       "      <td>19/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist F/H</td>\n",
       "      <td>YSANCE</td>\n",
       "      <td>A négocier</td>\n",
       "      <td>En tant que Data Scientist, vous contribuerez ...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Levallois-Perret - 92</td>\n",
       "      <td>21/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist F/H</td>\n",
       "      <td>METEOJOB</td>\n",
       "      <td>A négocier</td>\n",
       "      <td>A la recherche de nouvelles affinités professi...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Écully - 69</td>\n",
       "      <td>10/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist F/H</td>\n",
       "      <td>METEOJOB</td>\n",
       "      <td>A négocier</td>\n",
       "      <td>Rattaché(e) à la Direction Générale Exécutive ...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Clichy - 92</td>\n",
       "      <td>23/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist F/H</td>\n",
       "      <td>SAS PROXIEL</td>\n",
       "      <td>A partir de 40 k€ brut annuel</td>\n",
       "      <td>Nous recherchons un Data Scientist (F/H) pour ...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Nice - 06</td>\n",
       "      <td>09/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Scientist - F/H</td>\n",
       "      <td>ANSACONSULT</td>\n",
       "      <td>A négocier</td>\n",
       "      <td>Votre mission si vous l’acceptez : Vous accomp...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Lille - 59</td>\n",
       "      <td>24/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Scientist F/H</td>\n",
       "      <td>GROUPAMA ASSURANCES MUTUELLES</td>\n",
       "      <td>A partir de 20 k€ brut annuel</td>\n",
       "      <td>Au sein de la Direction Etudes Technique et Ta...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Nanterre - 92</td>\n",
       "      <td>16/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Scientist F/H</td>\n",
       "      <td>ABIL INTERIM</td>\n",
       "      <td>35 - 38 k€ brut annuel</td>\n",
       "      <td>Vous travaillez sous la responsabilité du mana...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Montauban - 82</td>\n",
       "      <td>29/10/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Scientist F/H</td>\n",
       "      <td>DASSAULT AVIATION</td>\n",
       "      <td>A négocier</td>\n",
       "      <td>La Direction Générale du Soutien Militaire (DG...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Saint-Cloud - 92</td>\n",
       "      <td>05/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data Scientist F/H</td>\n",
       "      <td>AGAP2IT</td>\n",
       "      <td>A partir de 35 k€ brut annuel</td>\n",
       "      <td>Au sein d'une équipe de 10 personnes, vous ser...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Boulogne-Billancourt - 92</td>\n",
       "      <td>08/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data Scientist F/H</td>\n",
       "      <td>CLEEVEN SE</td>\n",
       "      <td>25 - 40 k€ brut annuel</td>\n",
       "      <td>Au sein de nos équipes de data science, nous r...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Biot - 06</td>\n",
       "      <td>24/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Data Scientist F/H</td>\n",
       "      <td>REGIONSJOB</td>\n",
       "      <td>A négocier</td>\n",
       "      <td>Société indépendante comprenant 730 collaborat...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Villeurbanne - 69</td>\n",
       "      <td>26/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data scientist F/H</td>\n",
       "      <td>TALENTS IT</td>\n",
       "      <td>35 - 40 k€ brut annuel</td>\n",
       "      <td>LA SOCIÉTÉ Talents IT, cabinet de recrutement ...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Villeurbanne - 69</td>\n",
       "      <td>15/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data scientist F/H</td>\n",
       "      <td>TALENTS IT</td>\n",
       "      <td>35 - 40 k€ brut annuel</td>\n",
       "      <td>LA SOCIÉTÉ Talents IT, cabinet de recrutement ...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Villeurbanne - 69</td>\n",
       "      <td>09/11/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data Scientist F/H</td>\n",
       "      <td>CTS CORPORATE</td>\n",
       "      <td>30 - 35 k€ brut annuel</td>\n",
       "      <td>Dans le cadre du développement de nos activité...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Marignane - 13</td>\n",
       "      <td>23/11/2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Job title                        Company  \\\n",
       "0     Data Scientist F/H           FULL DATA MANAGEMENT   \n",
       "1     Data Scientist F/H          ROBERT WALTERS FRANCE   \n",
       "2     Data Scientist F/H                     TALENTS RH   \n",
       "3     DATA SCIENTIST F/H         STUDIEL PARTICIPATIONS   \n",
       "4     Data Scientist F/H                     REGIONSJOB   \n",
       "5     DATA SCIENTIST F/H               PREM CANAGARADJA   \n",
       "6     Data Scientist F/H                         YSANCE   \n",
       "7     Data Scientist F/H                       METEOJOB   \n",
       "8     Data Scientist F/H                       METEOJOB   \n",
       "9     Data Scientist F/H                    SAS PROXIEL   \n",
       "10  Data Scientist - F/H                    ANSACONSULT   \n",
       "11    Data Scientist F/H  GROUPAMA ASSURANCES MUTUELLES   \n",
       "12    Data Scientist F/H                   ABIL INTERIM   \n",
       "13    Data Scientist F/H              DASSAULT AVIATION   \n",
       "14    Data Scientist F/H                        AGAP2IT   \n",
       "15    Data Scientist F/H                     CLEEVEN SE   \n",
       "16    Data Scientist F/H                     REGIONSJOB   \n",
       "17    Data scientist F/H                     TALENTS IT   \n",
       "18    Data scientist F/H                     TALENTS IT   \n",
       "19    Data Scientist F/H                  CTS CORPORATE   \n",
       "\n",
       "                           Salary  \\\n",
       "0          35 - 45 k€ brut annuel   \n",
       "1          45 - 50 k€ brut annuel   \n",
       "2          45 - 50 k€ brut annuel   \n",
       "3          35 - 43 k€ brut annuel   \n",
       "4                      A négocier   \n",
       "5                      A négocier   \n",
       "6                      A négocier   \n",
       "7                      A négocier   \n",
       "8                      A négocier   \n",
       "9   A partir de 40 k€ brut annuel   \n",
       "10                     A négocier   \n",
       "11  A partir de 20 k€ brut annuel   \n",
       "12         35 - 38 k€ brut annuel   \n",
       "13                     A négocier   \n",
       "14  A partir de 35 k€ brut annuel   \n",
       "15         25 - 40 k€ brut annuel   \n",
       "16                     A négocier   \n",
       "17         35 - 40 k€ brut annuel   \n",
       "18         35 - 40 k€ brut annuel   \n",
       "19         30 - 35 k€ brut annuel   \n",
       "\n",
       "                                          Description Contract  \\\n",
       "0   Dans le cadre de notre développement, nous rec...      CDI   \n",
       "1   Notre client, groupe spécialisé dans l'assuran...      CDI   \n",
       "2   TALENTS RH, société de recrutement spécialisée...      CDI   \n",
       "3   Dans le cadre d'une embauche, vous interviendr...      CDI   \n",
       "4   \"Recrutez au delà des compétences.\" PERSUADERS...      CDI   \n",
       "5   D’une manière générale, vous serez en charge d...      CDI   \n",
       "6   En tant que Data Scientist, vous contribuerez ...      CDI   \n",
       "7   A la recherche de nouvelles affinités professi...      CDI   \n",
       "8   Rattaché(e) à la Direction Générale Exécutive ...      CDI   \n",
       "9   Nous recherchons un Data Scientist (F/H) pour ...      CDI   \n",
       "10  Votre mission si vous l’acceptez : Vous accomp...      CDI   \n",
       "11  Au sein de la Direction Etudes Technique et Ta...      CDI   \n",
       "12  Vous travaillez sous la responsabilité du mana...      CDI   \n",
       "13  La Direction Générale du Soutien Militaire (DG...      CDI   \n",
       "14  Au sein d'une équipe de 10 personnes, vous ser...      CDI   \n",
       "15  Au sein de nos équipes de data science, nous r...      CDI   \n",
       "16  Société indépendante comprenant 730 collaborat...      CDI   \n",
       "17  LA SOCIÉTÉ Talents IT, cabinet de recrutement ...      CDI   \n",
       "18  LA SOCIÉTÉ Talents IT, cabinet de recrutement ...      CDI   \n",
       "19  Dans le cadre du développement de nos activité...      CDI   \n",
       "\n",
       "                     Location        date  \n",
       "0                  Lille - 59  16/11/2021  \n",
       "1                 Toulon - 83  10/11/2021  \n",
       "2                  Lille - 59  18/11/2021  \n",
       "3                Blagnac - 31  16/11/2021  \n",
       "4                Lyon 01 - 69  26/11/2021  \n",
       "5               Toulouse - 31  19/11/2021  \n",
       "6       Levallois-Perret - 92  21/11/2021  \n",
       "7                 Écully - 69  10/11/2021  \n",
       "8                 Clichy - 92  23/11/2021  \n",
       "9                   Nice - 06  09/11/2021  \n",
       "10                 Lille - 59  24/11/2021  \n",
       "11              Nanterre - 92  16/11/2021  \n",
       "12             Montauban - 82  29/10/2021  \n",
       "13           Saint-Cloud - 92  05/11/2021  \n",
       "14  Boulogne-Billancourt - 92  08/11/2021  \n",
       "15                  Biot - 06  24/11/2021  \n",
       "16          Villeurbanne - 69  26/11/2021  \n",
       "17          Villeurbanne - 69  15/11/2021  \n",
       "18          Villeurbanne - 69  09/11/2021  \n",
       "19             Marignane - 13  23/11/2021  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prin the 20 first rows\n",
    "df.head(20) # 15 firts offers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
